{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Standard Deviation (LocalSTD) Anomaly Detection for Spacecraft Telemetry\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "The LocalSTD algorithm detects anomalies by analyzing local statistical properties within sliding windows, rather than comparing against global statistics. This approach is particularly effective for:\n",
    "- Time-series data with changing baselines\n",
    "- Systems with different operational modes\n",
    "- Detecting contextual anomalies that might appear normal globally\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Sliding Window**: Examines data in local neighborhoods\n",
    "2. **Local Statistics**: Computes mean and standard deviation within each window\n",
    "3. **Adaptive Thresholding**: Anomaly thresholds adapt to local context\n",
    "4. **Multi-channel Support**: Applies detection across multiple telemetry channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, \n",
    "    recall_score, confusion_matrix, fbeta_score\n",
    ")\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Space Dataset\n",
    "\n",
    "The dataset contains telemetry readings from multiple spacecraft channels. Each channel represents different sensor measurements or system parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train = pd.read_csv('X_train.csv')\n",
    "test = pd.read_csv('X_test.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Training set shape: {train.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")\n",
    "print(f\"\\nColumns in dataset:\")\n",
    "print(train.columns.tolist()[:10], \"...\")  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the LocalSTD Algorithm\n",
    "\n",
    "### How LocalSTD Works:\n",
    "\n",
    "1. **Window Definition**: For each data point, we define a local window of size `window_size`\n",
    "2. **Local Statistics**: Within each window, we compute:\n",
    "   - Local mean (μ_local)\n",
    "   - Local standard deviation (σ_local)\n",
    "3. **Anomaly Score**: For each point x_i:\n",
    "   - Score = |x_i - μ_local| / σ_local\n",
    "4. **Thresholding**: Points with scores > threshold are flagged as anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalSTDDetector:\n",
    "    \"\"\"\n",
    "    Local Standard Deviation Anomaly Detector\n",
    "    \n",
    "    This detector identifies anomalies by comparing each point to its local neighborhood\n",
    "    statistics rather than global statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=50, threshold=3.0, min_std=1e-6):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        window_size : int\n",
    "            Size of the sliding window for local statistics\n",
    "        threshold : float\n",
    "            Number of standard deviations from local mean to flag as anomaly\n",
    "        min_std : float\n",
    "            Minimum standard deviation to prevent division by zero\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        self.min_std = min_std\n",
    "        \n",
    "    def compute_local_statistics(self, data):\n",
    "        \"\"\"\n",
    "        Compute rolling mean and standard deviation for each channel\n",
    "        \"\"\"\n",
    "        # Calculate rolling statistics with centered window\n",
    "        rolling_mean = data.rolling(\n",
    "            window=self.window_size, \n",
    "            center=True, \n",
    "            min_periods=1\n",
    "        ).mean()\n",
    "        \n",
    "        rolling_std = data.rolling(\n",
    "            window=self.window_size, \n",
    "            center=True, \n",
    "            min_periods=1\n",
    "        ).std()\n",
    "        \n",
    "        # Replace NaN and zero std with minimum value\n",
    "        rolling_std = rolling_std.fillna(self.min_std)\n",
    "        rolling_std[rolling_std < self.min_std] = self.min_std\n",
    "        \n",
    "        return rolling_mean, rolling_std\n",
    "    \n",
    "    def compute_anomaly_scores(self, data, rolling_mean, rolling_std):\n",
    "        \"\"\"\n",
    "        Compute z-scores based on local statistics\n",
    "        \"\"\"\n",
    "        z_scores = np.abs((data - rolling_mean) / rolling_std)\n",
    "        return z_scores\n",
    "    \n",
    "    def detect_anomalies(self, data, channels=None):\n",
    "        \"\"\"\n",
    "        Detect anomalies in the specified channels\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Input data\n",
    "        channels : list\n",
    "            List of channel names to analyze\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        anomalies : pd.DataFrame\n",
    "            Boolean DataFrame indicating anomalies\n",
    "        scores : pd.DataFrame\n",
    "            Anomaly scores for each point\n",
    "        \"\"\"\n",
    "        if channels is None:\n",
    "            channels = [col for col in data.columns if col.startswith('channel_')]\n",
    "        \n",
    "        anomalies = pd.DataFrame(index=data.index)\n",
    "        scores = pd.DataFrame(index=data.index)\n",
    "        \n",
    "        for channel in channels:\n",
    "            if channel in data.columns:\n",
    "                # Compute local statistics\n",
    "                rolling_mean, rolling_std = self.compute_local_statistics(data[channel])\n",
    "                \n",
    "                # Compute anomaly scores\n",
    "                channel_scores = self.compute_anomaly_scores(\n",
    "                    data[channel], rolling_mean, rolling_std\n",
    "                )\n",
    "                \n",
    "                # Flag anomalies\n",
    "                channel_anomalies = channel_scores > self.threshold\n",
    "                \n",
    "                anomalies[channel] = channel_anomalies\n",
    "                scores[channel] = channel_scores\n",
    "        \n",
    "        return anomalies, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply LocalSTD to Critical Channels\n",
    "\n",
    "We'll focus on channels 41-46, which appear to be critical telemetry channels based on the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define critical channels to monitor\n",
    "critical_channels = [f\"channel_{i}\" for i in range(41, 47)]\n",
    "print(f\"Monitoring channels: {critical_channels}\")\n",
    "\n",
    "# Initialize the detector with optimized parameters\n",
    "detector = LocalSTDDetector(\n",
    "    window_size=100,  # Adjust based on expected anomaly duration\n",
    "    threshold=4.0     # Stricter threshold for space applications\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model (Parameter Tuning)\n",
    "\n",
    "Since we're using an unsupervised method, \"training\" involves:\n",
    "1. Analyzing the training data to understand normal behavior\n",
    "2. Tuning parameters based on the characteristics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training data to understand normal behavior\n",
    "train_subset = train[critical_channels]\n",
    "\n",
    "# Compute statistics for parameter tuning\n",
    "train_stats = train_subset.describe()\n",
    "print(\"Training Data Statistics:\")\n",
    "print(train_stats.T[['mean', 'std', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of one channel to understand the data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, channel in enumerate(critical_channels):\n",
    "    axes[idx].hist(train[channel].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{channel} Distribution')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Detect Anomalies in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LocalSTD detection to test data\n",
    "test_anomalies, test_scores = detector.detect_anomalies(\n",
    "    test, \n",
    "    channels=critical_channels\n",
    ")\n",
    "\n",
    "# Aggregate results: flag as anomaly if ANY channel shows anomaly\n",
    "aggregated_anomalies = test_anomalies.any(axis=1).astype(int)\n",
    "\n",
    "print(f\"Total anomalies detected: {aggregated_anomalies.sum()}\")\n",
    "print(f\"Anomaly rate: {aggregated_anomalies.mean():.2%}\")\n",
    "print(f\"\\nAnomalies per channel:\")\n",
    "print(test_anomalies.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomaly detection results over time\n",
    "fig, axes = plt.subplots(len(critical_channels) + 1, 1, \n",
    "                         figsize=(15, 2.5 * (len(critical_channels) + 1)))\n",
    "\n",
    "# Plot each channel with anomalies highlighted\n",
    "for idx, channel in enumerate(critical_channels):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot normal data\n",
    "    ax.plot(test.index, test[channel], 'b-', alpha=0.6, linewidth=0.5, label='Normal')\n",
    "    \n",
    "    # Highlight anomalies\n",
    "    anomaly_mask = test_anomalies[channel]\n",
    "    if anomaly_mask.any():\n",
    "        ax.scatter(test.index[anomaly_mask], test[channel][anomaly_mask], \n",
    "                  c='red', s=10, alpha=0.8, label='Anomaly')\n",
    "    \n",
    "    ax.set_title(f'{channel} - LocalSTD Detection')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot aggregated results\n",
    "axes[-1].fill_between(test.index, 0, aggregated_anomalies, \n",
    "                      color='red', alpha=0.3, step='mid')\n",
    "axes[-1].set_title('Aggregated Anomaly Detection (Any Channel)')\n",
    "axes[-1].set_xlabel('Sample Index')\n",
    "axes[-1].set_ylabel('Anomaly Flag')\n",
    "axes[-1].set_ylim(-0.1, 1.1)\n",
    "axes[-1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Anomaly Scores Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of anomaly scores\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, channel in enumerate(critical_channels):\n",
    "    scores = test_scores[channel]\n",
    "    \n",
    "    axes[idx].hist(scores, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].axvline(detector.threshold, color='red', linestyle='--', \n",
    "                     linewidth=2, label=f'Threshold={detector.threshold}')\n",
    "    axes[idx].set_title(f'{channel} Anomaly Scores')\n",
    "    axes[idx].set_xlabel('Z-Score (Local)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics text\n",
    "    stats_text = f'Max: {scores.max():.2f}\\n95%: {scores.quantile(0.95):.2f}'\n",
    "    axes[idx].text(0.7, 0.9, stats_text, transform=axes[idx].transAxes,\n",
    "                  bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Performance Evaluation (if ground truth is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If ground truth labels are available in the test set\n",
    "if 'is_anomaly' in test.columns:\n",
    "    y_true = test['is_anomaly'].values\n",
    "    y_pred = aggregated_anomalies.values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    f05 = fbeta_score(y_true, y_pred, beta=0.5)  # Favor precision\n",
    "    f2 = fbeta_score(y_true, y_pred, beta=2.0)   # Favor recall\n",
    "    \n",
    "    print(\"Performance Metrics:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"F0.5 Score (precision-focused): {f05:.4f}\")\n",
    "    print(f\"F2 Score (recall-focused):      {f2:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - LocalSTD Detection')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No ground truth labels available for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Parameter Sensitivity Analysis\n",
    "\n",
    "Understanding how different parameters affect detection performance is crucial for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different parameter combinations\n",
    "window_sizes = [50, 100, 200]\n",
    "thresholds = [3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "for window in window_sizes:\n",
    "    for thresh in thresholds:\n",
    "        # Create detector with current parameters\n",
    "        temp_detector = LocalSTDDetector(window_size=window, threshold=thresh)\n",
    "        \n",
    "        # Detect anomalies\n",
    "        temp_anomalies, _ = temp_detector.detect_anomalies(test, channels=critical_channels)\n",
    "        temp_aggregated = temp_anomalies.any(axis=1).astype(int)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'window_size': window,\n",
    "            'threshold': thresh,\n",
    "            'anomaly_count': temp_aggregated.sum(),\n",
    "            'anomaly_rate': temp_aggregated.mean()\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot_table = results_df.pivot(index='threshold', columns='window_size', values='anomaly_rate')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "plt.title('Anomaly Detection Rate by Parameters')\n",
    "plt.xlabel('Window Size')\n",
    "plt.ylabel('Threshold (σ)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'is_anomaly': aggregated_anomalies\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv('localstd_submission.csv', index=False)\n",
    "print(f\"Submission file saved: localstd_submission.csv\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "### LocalSTD vs GlobalSTD:\n",
    "\n",
    "**LocalSTD Advantages:**\n",
    "- Adapts to changing baselines in the data\n",
    "- Better at detecting contextual anomalies\n",
    "- More robust to concept drift\n",
    "- Suitable for non-stationary time series\n",
    "\n",
    "**GlobalSTD Advantages:**\n",
    "- Simpler to implement and interpret\n",
    "- Lower computational cost\n",
    "- Better for detecting global outliers\n",
    "- Suitable for stationary data\n",
    "\n",
    "### Application to Space Data:\n",
    "\n",
    "For spacecraft telemetry:\n",
    "1. **LocalSTD is preferred** when:\n",
    "   - Spacecraft operates in different modes (launch, cruise, orbit)\n",
    "   - Environmental conditions change (eclipse, solar radiation)\n",
    "   - Gradual degradation needs to be distinguished from sudden faults\n",
    "\n",
    "2. **Key Considerations:**\n",
    "   - Window size should match expected anomaly duration\n",
    "   - Threshold tuning is critical for balancing false positives/negatives\n",
    "   - Multi-channel aggregation strategy affects overall sensitivity\n",
    "\n",
    "### Recommendations:\n",
    "- Start with window_size = 100 and threshold = 4.0\n",
    "- Monitor critical channels more strictly\n",
    "- Consider ensemble methods combining LocalSTD and GlobalSTD\n",
    "- Implement alert prioritization based on anomaly scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}